# [] 往往是layer name
[net]
# Testing
#batch=1
#subdivisions=1
# Training
# batch & subdivision: real training size each time: batch/subdivisions
# yolov4 creates a newer CmBN method based on this)
batch=16
subdivisions=1
width=416
height=416
channels=3
momentum=0.9
decay=0.0005
# Optimizer:
# please remember SGD(+ Momentum), Nesterov, RMSProp || Adagrad, Adam & then:
# Better to know at least one of the rest.
#
# AdamW: https://arxiv.org/pdf/1711.05101.pdf
# Nadam: http://cs229.stanford.edu/proj2015/054_report.pdf [not a paper, a course report]
# RAdam: https://arxiv.org/pdf/1908.03265.pdf
# LookAhead: https://arxiv.org/abs/1907.08610
# Ranger: (RAdam + LookAhead + GC [Gradient Centralization]) https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer
#         work with Mish (activation function) perfectly
# LAMB (BERT, Both large(>1k) & regular batch size): https://arxiv.org/pdf/1904.00962.pdf

# for data augmentation (angle is not used in program): color shifting
# jitter=.3 (in [yolo] section) is also used for data augmentation: size of original image
angle=0
saturation = 1.5
exposure = 1.5
hue=.1

learning_rate=0.001
# BURN_IN:
# Worked in original Darknet. We may have training policy to change the lr.
# We start this procedure after burn_in. So, before burn_in: policy0, after burn_in: policy we set (like multi-step)
# Usually, we set lr from 0 to initial lr during burn-in
#
# If we use SGD, a good implementation:
# https://github.com/ultralytics/yolov3/commit/a722601ef61149cc9e5135f58c762310627c970a [line 115-124]
#
# Another good example to burn-in (here we use: warmup, same thing):
# Faceboxes: https://github.com/zisianw/FaceBoxes.PyTorch/blob/master/train.py (from line 137)
#
# So, initial LR you set should not be too far away from the last value after burn-in
#
# Why burn-in:
# RAdam's explanation: The root issue is that adaptive learning rate optimizers have too large of a variance,
# especially in the early stages of training, and make excessive jumps based on limited training data. Therefore,
# warmup serves as a variance reduce
# [Cannot explain why SGD+warmup can also present better effect]
# [Just like you enter in a new environment, you also need time to warmup]
# [The use of burn-in/warmup can be dated back to ResNet)]
burn_in=1000
# warm up
max_batches = 500200
policy=steps
steps=400000,450000
scales=.1,.1

# A very good figural explanation:
# https://blog.csdn.net/dz4543/article/details/90049377
# layer 0
[convolutional]
batch_normalize=1
filters=32
size=3
stride=1
pad=1
# leaky=leaky relu
activation=leaky

# Downsample
# layer 1
[convolutional]
batch_normalize=1
filters=64
size=3
stride=2
pad=1
activation=leaky
# layer 2
[convolutional]
batch_normalize=1
filters=32
size=1
stride=1
pad=1
activation=leaky
# layer 3
[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky
# 4 = 1(4-3) + 3(上一层)
[shortcut]
# output = network[-3] + network[shortcut/current]
# linear: do nothing. 占位用
from=-3
activation=linear

# Downsample
# 5
[convolutional]
batch_normalize=1
filters=128
size=3
stride=2
pad=1
activation=leaky
# 6
[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=leaky
# 7
[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky
# 8 = 5 + 7
[shortcut]
from=-3
activation=linear
# 9
[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=leaky
# 10
[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky
# 11 = 8 + 10
[shortcut]
from=-3
activation=linear

# Downsample
# 12
[convolutional]
batch_normalize=1
filters=256
size=3
stride=2
pad=1
activation=leaky
# 13
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
# 14
[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky
# 15 = 12 + 14
[shortcut]
from=-3
activation=linear
# 16
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
# 17
[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky
# 18 = 15 + 17
[shortcut]
from=-3
activation=linear
# 19
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
# 20
[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky
# 21 = 18 + 20
[shortcut]
from=-3
activation=linear
# 22
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
# 23
[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky
# 24 = 21 + 23
[shortcut]
from=-3
activation=linear
# 25
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
# 26
[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky
# 27 = 24 + 26
[shortcut]
from=-3
activation=linear
# 28
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
# 29
[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky
# 30 = 27 + 29
[shortcut]
from=-3
activation=linear
# 31
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
# 32
[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky
# 33 = 30 + 32
[shortcut]
from=-3
activation=linear
# 34
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
# 35
[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky
# 36 = 33 + 35
[shortcut]
from=-3
activation=linear

# Downsample
# 37
[convolutional]
batch_normalize=1
filters=512
size=3
stride=2
pad=1
activation=leaky
# 38
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
# 39
[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky
# 40 = 37 + 39
[shortcut]
from=-3
activation=linear

# 41
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
# 42
[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky
# 43 = 40 + 42
[shortcut]
from=-3
activation=linear

# 44
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
# 45
[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky
# 46 = 43 + 45
[shortcut]
from=-3
activation=linear

# 47
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
# 48
[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky
# 49 = 46 + 48
[shortcut]
from=-3
activation=linear
# 50
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
# 51
[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky
# 52 = 49 + 51
[shortcut]
from=-3
activation=linear

# 53
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
# 54
[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky
# 55 = 52 + 54
[shortcut]
from=-3
activation=linear

# 56
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
# 57
[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky
# 58 = 55 + 57
[shortcut]
from=-3
activation=linear
# 59
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
# 60
[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky
# 61 = 58 + 60
[shortcut]
from=-3
activation=linear

# Downsample
# 62
[convolutional]
batch_normalize=1
filters=1024
size=3
stride=2
pad=1
activation=leaky
# 63
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky
# 64
[convolutional]
batch_normalize=1
filters=1024
size=3
stride=1
pad=1
activation=leaky
# 65 = 62 + 64
[shortcut]
from=-3
activation=linear
# 66
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky
# 67
[convolutional]
batch_normalize=1
filters=1024
size=3
stride=1
pad=1
activation=leaky
# 68 = 65 + 67
[shortcut]
from=-3
activation=linear
# 69
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky
# 70
[convolutional]
batch_normalize=1
filters=1024
size=3
stride=1
pad=1
activation=leaky
# 71 = 68 + 70
[shortcut]
from=-3
activation=linear
# 72
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky
# 73
[convolutional]
batch_normalize=1
filters=1024
size=3
stride=1
pad=1
activation=leaky
# 74 = 71  + 73
[shortcut]
from=-3
activation=linear

######################
# 75
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky
# 76
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=1024
activation=leaky
# 77
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky
# 78
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=1024
activation=leaky
# 79
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky
# 80
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=1024
activation=leaky
# 81
[convolutional]
size=1
stride=1
pad=1
# (classes + 1+ coords) * anchors_num = 85 * 3 = 255
filters=255
activation=linear

# 82
[yolo]
# 13x13 featurem map,
# corresponding bigger object.
# Corresponding last 3 anchors
mask = 6,7,8
anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326
classes=80
num=9
jitter=.3
# will be reset to 0.5 in models.py/YOLOLayer.init [will revisit later]
ignore_thresh = .7
truth_thresh = 1
# size changed from 320-608 randomly
random=1

# 83 = 79
[route]
# go back to layer[-4]
# output(layer[-4])
layers = -4
# 84 (接从83，实际就是从79接)
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
# 85
[upsample]
stride=2
# 86 = concat(85, 61)
[route]
# output = concat(output(layers[-1]), output(layers[61]))
layers = -1, 61


# 87
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
# 88
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=512
activation=leaky
# 89
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
# 90
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=512
activation=leaky
# 91
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
# 92
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=512
activation=leaky
# 93
[convolutional]
size=1
stride=1
pad=1
filters=255
activation=linear

# 94: 26 x 26 x 255, corresponding to the mid 3 anchors
[yolo]
mask = 3,4,5
anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326
classes=80
num=9
jitter=.3
ignore_thresh = .7
truth_thresh = 1
random=1


# 95 = 90
[route]
layers = -4
# 96 (从95，也就是90开始接)
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
# 97
[upsample]
stride=2
# 98 = concat(97, 36)
[route]
layers = -1, 36


# 99
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
# 100
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=256
activation=leaky
# 101
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
# 102
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=256
activation=leaky
# 103
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
# 104
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=256
activation=leaky
# 105
[convolutional]
size=1
stride=1
pad=1
filters=255
activation=linear

# 106: 52 x 52 x 255, corresponding to first 3 anchors
[yolo]
mask = 0,1,2
anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326
classes=80
num=9
jitter=.3
ignore_thresh = .7
truth_thresh = 1
random=1
